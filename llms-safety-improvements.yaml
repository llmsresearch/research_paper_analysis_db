papers:
  - date: "29th April, 2024"
    title: "A Framework for Real-time Safeguarding the Text Generation of Large Language"
    description: "LLMSafeGuard is a framework that integrates an external validator into the beam search algorithm during decoding, rejecting candidates that violate safety constraints while allowing valid ones to proceed."
    details: "The research paper proposes a lightweight framework called LLMSafeGuard to safeguard LLM text generation in real-time.
               It integrates an external validator into the beam search algorithm during decoding, rejecting candidates that violate safety 
               constraints while allowing valid ones to proceed. This approach simplifies constraint introduction and eliminates the need for 
               training specific control models. LLMSafeGuard also employs a context-wise timing selection strategy, intervening LLMs only when necessary."
    tags: ["attack", "jailbreak", "framework"]
    url: "http://arxiv.org/abs/2404.19048v1"
    image: ["https://github.com/llmsresearch/research_paper_analysis_db/images/29th-april-2024/llmsafeguard.png"]
  - date: "30th April, 2024"
    title: "Enhancing Trust in LLM-Generated Code Summaries with Calibrated Confidence Scores"
    description: "Considers summarization as a calibration problem and provides a predictions of the likelihood of similarity to human values"
    details: "The research paper proposes a solution which studies summarization as a calibration problem. It suggests using a confidence measure to determine whether a summary generated by an AI-based method is likely to be similar to what a human would produce. This is achieved by examining the performance of several LLMs in different settings and languages. The paper suggests an approach that provides well-calibrated predictions of the likelihood of similarity to human summaries."
    tags: ["framework", "AI detector"]
    url: "http://arxiv.org/abs/2404.19318v1"
    image: []
  - date: "30th April, 2024"
    title: "Safe Training with Sensitive In-domain Data: Leveraging Data Fragmentation To Mitigate Linkage Attacks"
    description: "Paper prevents exposing sensitive information in LLMs text generation. It provides a safer alternative where the data is fragmented into short phrases that are randomly grouped together and shared instead of full texts."
    tags: ["safeguard response"]
    url: "http://arxiv.org/abs/2404.19486v1"
    details: "Paper proposes a safer alternative where the data is fragmented into short phrases that are randomly grouped together and shared instead of full texts. This prevents the model from memorizing and reproducing sensitive information in one sequence, thus protecting against linkage attacks. The authors fine-tune several state-of-the-art LLMs using meaningful syntactic chunks to explore their utility and demonstrate the effectiveness of this approach."
    image: []
  - date: "30th April, 2024"
    title: "Transferring Troubles: Cross-Lingual Transferability of Backdoor Attacks in LLMs with Instruction Tuning "
    description: "Paper investigate backdoor attacks agains multilingual LLMs. This involves poisoning the instruction-tuning data in one or two languages, which can then activate malicious outputs in languages whose instruction-tuning data was not poisoned."
    tags: ["attack"]
    url: "http://arxiv.org/abs/2404.19597v1"
    details: "The research paper addresses the issue of backdoor attacks on English-centric LLMs and how they can also affect multilingual models. Paper investigates cross-lingual backdoor attacks against multilingual LLMs. This involves poisoning the instruction-tuning data in one or two languages, which can then activate malicious outputs in languages whose instruction-tuning data was not poisoned. This method has been shown to have a high attack success rate in various scenarios, even after paraphrasing, and can work across 25 languages."
    image: ["https://github.com/llmsresearch/research_paper_analysis_db/images/30th-april-2024/transferring-troubles.png"]